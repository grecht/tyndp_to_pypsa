{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from geopy.geocoders import Nominatim\n",
    "from geopy import *\n",
    "import geopy.distance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import re\n",
    "import Levenshtein"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: what about voltage level?\n",
    "# TODO: somehow map statuses to some sort of enum\n",
    "# TODO: map column names to fixed names (see example csv)\n",
    "\n",
    "#### 2020 ####\n",
    "column_semantics_2020 = {\n",
    "    'Investment number': 'investment_id',\n",
    "    'Commissioning Year': 'commissioning_year',\n",
    "    'Status ID\\n1 : Under Consideration,\\n2 : In Planning but not permitting,\\n3 : In permitting,\\n4 : Under Construction': 'status',\n",
    "    'Type of Element': 'asset_type',\n",
    "    'Substation From': 'substation_1',\n",
    "    'Substation To': 'substation_2',\n",
    "    'Technology': 'current_type',\n",
    "    'Total route length (km)': 'specified_length_km'\n",
    "}\n",
    "\n",
    "status_map_2020 = {\n",
    "    1: 1, # under consideration\n",
    "    2: 2, # planning, not permitting\n",
    "    3: 3, # in permitting\n",
    "    4: 4  # under construction\n",
    "}\n",
    "params_2020 = {\n",
    "    'excel': '2020/transmission.xlsx',\n",
    "    'sheet': 'Trans.Investments',\n",
    "    'header_row': 1,\n",
    "    'status_column': 'Status ID\\n1 : Under Consideration,\\n2 : In Planning but not permitting,\\n3 : In permitting,\\n4 : Under Construction',\n",
    "    'status_map': status_map_2020,\n",
    "    'column_semantics': column_semantics_2020\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#### 2018 ####\n",
    "column_semantics_2018 = {\n",
    "    'Investment ID': 'investment_id',\n",
    "    'ExpectedCommissioningYear': 'commissioning_year',\n",
    "    'Status': 'status',\n",
    "    'ElementsType': 'asset_type',\n",
    "    'From': 'substation_1',\n",
    "    'To': 'substation_2',\n",
    "    'TechnologyType': 'current_type',\n",
    "    'TotalRouteLength (km)': 'specified_length_km'\n",
    "}\n",
    "\n",
    "status_map_2018 = {\n",
    "    'under consideration': 1,\n",
    "    'planned but not yet permitting': 2,\n",
    "    'permitting': 3,\n",
    "    'under construction': 4\n",
    "}\n",
    "\n",
    "params_2018 = {\n",
    "    'excel': r'2018/TYNDP_2018_Project_List.xlsx',\n",
    "    'sheet': 'Sheet1',\n",
    "    'header_row': 0,\n",
    "    'status_column': 'Status',\n",
    "    'status_map': status_map_2018,\n",
    "    'column_semantics': column_semantics_2018\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = params_2018"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "excel = params['excel']\n",
    "sheet = params['sheet']\n",
    "\n",
    "column_semantics = params['column_semantics']\n",
    "\n",
    "wanted_columns = column_semantics.keys()\n",
    "status_column  = [k for (k,v) in column_semantics.items() if v == 'status'][0]\n",
    "status_map = params['status_map']\n",
    "header_row = params['header_row']\n",
    "\n",
    "wanted = pd.read_excel(excel, sheet_name=sheet, header=header_row)[wanted_columns]\n",
    "\n",
    "# map columns to specified names (-> consistency & semantics)\n",
    "wanted.columns = [column_semantics[c] for c in wanted.columns]\n",
    "\n",
    "if wanted['status'].dtype == pd.StringDtype:\n",
    "    wanted['status'] = wanted['status'].str.lower()\n",
    "\n",
    "# only keep rows whose status is specified in status_map as a key\n",
    "wanted = wanted.loc[wanted['status'].isin(status_map.keys())]\n",
    "\n",
    "# replace status with numerical values as specified in status_map\n",
    "wanted = wanted.replace({'status': status_map})\n",
    "\n",
    "\n",
    "# only choose those in permitting or under construction\n",
    "wanted = wanted.loc[wanted['status'].astype(int) >= 3]\n",
    "\n",
    "wanted.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: somehow make this work for every case\n",
    "\n",
    "#### 2020 ####\n",
    "# ac_lines  = wanted.query(\"`Type of Element` == 'ACTransmissionLine'\")\n",
    "# dc_lines  = wanted.query(\"`Type of Element` == 'DCTransmissionLine'\")\n",
    "# on_subst  = wanted.query(\"`Type of Element` == 'OnshoreSubstation'\")\n",
    "# off_subst = wanted.query(\"`Type of Element` == 'OffshoreSubstation'\")\n",
    "# \n",
    "# lines = ac_lines.append(dc_lines)\n",
    "lines = wanted.loc[wanted['asset_type'] == 'Overhead Line']\n",
    "\n",
    "if params == params_2020:\n",
    "    ac_lines  = wanted.query(\"asset_type == 'ACTransmissionLine'\")\n",
    "    dc_lines  = wanted.query(\"asset_type == 'DCTransmissionLine'\")\n",
    "    on_subst  = wanted.query(\"asset_type == 'OnshoreSubstation'\")\n",
    "    off_subst = wanted.query(\"asset_type == 'OffshoreSubstation'\")\n",
    "\n",
    "    new_subst = set(on_subst['substation_1']).union(on_subst['substation_2'])\n",
    "    lines = ac_lines.append(dc_lines)\n",
    "elif params == params_2018:\n",
    "    lines = wanted.loc[wanted['asset_type'] == 'Overhead Line']\n",
    "    new_subst = set(wanted.loc[wanted['asset_type'] == 'Substation']['substation_1'])\n",
    "\n",
    "lines  = lines.query(\"substation_1 not in @new_subst\")\n",
    "lines  = lines.query(\"substation_2 not in @new_subst\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use bus names from buses.csv (v0.1.0)\n",
    "See https://github.com/PyPSA/pypsa-eur/blob/v0.1.0rc/data/entsoegridkit/buses.csv. Data is from 2017 (newer gridkit extracts do not contain 'tags' with substation names)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "buses_file = 'buses_v0.1.0.csv'\n",
    "\n",
    "# see base_network.py in PyPSA-Eur repository\n",
    "buses = (pd.read_csv(buses_file, quotechar=\"'\",\n",
    "                     true_values='t', false_values='f',\n",
    "                     dtype=dict(bus_id=\"str\"))\n",
    "        .set_index(\"bus_id\")\n",
    "        .drop(['station_id'], axis=1)\n",
    "        .rename(columns=dict(voltage='v_nom')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "buses = buses.query('tags.notnull()', engine='python')\n",
    "buses = buses.query(\"symbol == 'Substation'\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extract 'name_eng' and 'country' from tags in  buses"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "split_regex = r'(\"\\w+\"=>\"[^\"]*\"),' # Form: 'key => value, key => value, ...'\n",
    "\n",
    "tag_regex   = r'\"(?P<key>\\w+)\"=>\"(?P<value>[^\"]*)\"' # Form: 'key => value'\n",
    "tag_pattern = re.compile(tag_regex)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for index, row in buses.iterrows():\n",
    "    name    = ''\n",
    "    country = ''\n",
    "    x = row['x']\n",
    "    y = row['y']\n",
    "    \n",
    "    tags_string = row['tags']\n",
    "    \n",
    "    tags = re.split(split_regex, tags_string)\n",
    "    \n",
    "    # Remove whitespaces at front and end, remove None values\n",
    "    tags = [s.strip() for s in tags]\n",
    "    tags = list(filter(None, tags))\n",
    "    \n",
    "    for tag in tags:\n",
    "        m = tag_pattern.match(tag)\n",
    "            \n",
    "        if m is None:\n",
    "            print(tag)\n",
    "            \n",
    "        # see group names in tag_regex\n",
    "        key   = m.group('key')\n",
    "        value = m.group('value')\n",
    "        \n",
    "        if key == 'name_eng':\n",
    "            name = value.strip()\n",
    "        elif key == 'country':\n",
    "            country = value.strip()\n",
    "    \n",
    "    if name == 'unknown' or not name:\n",
    "        continue\n",
    "        \n",
    "    rows.append((name, country, x, y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "curated_buses = pd.DataFrame.from_records(rows, columns=['name', 'country', 'x', 'y'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove duplicate rows"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "curated_buses = curated_buses.loc[~curated_buses.duplicated()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## There are substations which share the same name but have different coordinates\n",
    "- large deviation between coordinates => substations are most likely in different countries \n",
    "    - BUT: it does occur that different places in the same country get the same name\n",
    "- small deviation between coordinates => reference to same substation (error in gridextract?)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: added 'NI' although Northern Ireland probably appears in PyPSA as 'GB'. Find a better solution.\n",
    "pypsa_countries = ['AL', 'AT', 'BA', 'BE', 'BG', 'CH', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GB', 'GR', 'HR', 'HU', 'IE', 'IT', 'LT', 'LU', 'LV', 'ME', 'MK', 'NI', 'NL', 'NO', 'PL', 'PT', 'RO', 'RS', 'SE', 'SI', 'SK']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### List of all duplicates"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "duplicated = curated_buses.loc[curated_buses.name.duplicated()]\n",
    "duplicated = duplicated.query(\"country in @pypsa_countries\")\n",
    "\n",
    "# for name in duplicated.name.unique():\n",
    "#     print(name)\n",
    "#     for index, row in curated_buses.query('name == @name').iterrows():\n",
    "#         print(f\"({row['x']}, {row['y']}), {row['country']}\")\n",
    "#     print('----')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Same name and country, large deviations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "curated_buses.query(\"name == 'Yuzhnaya'\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Same name, different country, large deviation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "curated_buses.query(\"name == 'Saida'\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "curated_buses.query(\"name == 'Titan'\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (TODO) Add new substations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# on_subst\n",
    "\n",
    "# extract country if it matches regex\n",
    "# otherwise, np.NAN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove '(\\<Country Code\\>) ' from tyndp substation name strings, add new column instead\n",
    "Otherwise, this could negatively impact the Levenshtein distance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "subst_regex   = r'(?P<place>.+)\\s?[\\[(](?P<country>\\w{2})[)\\]]' # Form: 'Glorenza (IT)'\n",
    "subst_pattern = re.compile(subst_regex)\n",
    "\n",
    "# TODO: does it make sense to \"throw away\" information here?\n",
    "# use this if other pattern does not match to remove comments in parentheses\n",
    "# e.g. 'Molai (through Sklavouna Terminal)'\n",
    "alt_regex   = r'(?P<place>.+)\\s?[\\[(].*[)\\]]'\n",
    "alt_pattern = re.compile(alt_regex)\n",
    "\n",
    "fr_names     = []\n",
    "fr_countries = []\n",
    "to_names     = []\n",
    "to_countries = []\n",
    "\n",
    "for index, row in lines.iterrows():    \n",
    "    fr = row['substation_1']\n",
    "    to = row['substation_2']\n",
    "    \n",
    "    # default values if regex does not match\n",
    "    fr_name = fr\n",
    "    to_name = to    \n",
    "    fr_country = np.NAN\n",
    "    to_country = np.NAN\n",
    "    \n",
    "    fr_match = subst_pattern.match(fr)\n",
    "    to_match = subst_pattern.match(to)\n",
    "    \n",
    "    if fr_match:\n",
    "        fr_name    = fr_match.group('place').strip()\n",
    "        fr_country = fr_match.group('country').strip()\n",
    "    else:\n",
    "        fr_alt_match = alt_pattern.match(fr)\n",
    "        if fr_alt_match:\n",
    "            fr_name = fr_alt_match.group('place')\n",
    "        \n",
    "    if to_match:\n",
    "        to_name    = to_match.group('place').strip()\n",
    "        to_country = to_match.group('country').strip()\n",
    "    else:        \n",
    "        to_alt_match = alt_pattern.match(to)\n",
    "        if to_alt_match:\n",
    "            to_name = to_alt_match.group('place')\n",
    "    \n",
    "    fr_names.append(fr_name)\n",
    "    fr_countries.append(fr_country)\n",
    "    to_names.append(to_name)\n",
    "    to_countries.append(to_country)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lines['substation_1'] = fr_names\n",
    "lines['substation_2'] = to_names\n",
    "lines['country_1'] = fr_countries\n",
    "lines['country_2'] = to_countries\n",
    "lines.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create mapping from all unique tyndp substation names to substation names from 'buses'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tyndp_subs   = set(lines['substation_1']).union(set(lines['substation_2']))\n",
    "tyndp_to_bus = {}\n",
    "\n",
    "for tyndp in tyndp_subs:\n",
    "    buses_subs = curated_buses.name.values\n",
    "    \n",
    "    closest = min([(bus, Levenshtein.distance(bus.lower(), tyndp.lower())) for bus in buses_subs], key=lambda t: t[1])[0]\n",
    "    print()\n",
    "    \n",
    "    tyndp_to_bus[tyndp] = closest"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# a, b = 'Turleenan', 'Guillena'\n",
    "# a, b = 'Pyhanselka', 'Pyhänselkä'\n",
    "# a, b = 'Tuomela B', 'Tudela'\n",
    "# a, b =  'Heviz (HU) \\\\ Zerjavinec', 'Žerjavinec'\n",
    "# Levenshtein.distance(a.lower(), b.lower())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tyndp_to_bus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper functions: Out of all possible pairs of locations from two lists, get the pair whose distance is closest to the specified (line) length\n",
    "Deals with problem of multiple places in same country sharing a name."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: keep name!\n",
    "def extract_coords(rows):\n",
    "    coordinates = []\n",
    "    for _, row in rows.iterrows():\n",
    "        coordinates.append((row['x'], row['y']))\n",
    "    return coordinates"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def match_pair_with_length(s1_rows, s2_rows, length):\n",
    "    s1_coords = extract_coords(s1_rows)\n",
    "    s2_coords = extract_coords(s2_rows)\n",
    "    \n",
    "    combinations  = list(itertools.product(s1_coords, s2_coords))\n",
    "    with_distance = [(a, b, geopy.distance.distance(a,b).km) for (a,b) in combinations]\n",
    "    \n",
    "    best_match = min(with_distance, key=lambda t: abs(length - t[2]))\n",
    "    return best_match"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Match start- and endpoints of lines to substations from buses.csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fr_to_tuples  = {}\n",
    "error_rows = {}\n",
    "\n",
    "for index, row in lines.iterrows():\n",
    "    # TODO: should we keep the original name here?\n",
    "    fr = row['substation_1']\n",
    "    to = row['substation_2']\n",
    "    \n",
    "    fr_country = row['country_1']\n",
    "    to_country = row['country_2']\n",
    "            \n",
    "    s1 = tyndp_to_bus[fr]\n",
    "    s2 = tyndp_to_bus[to]\n",
    "    \n",
    "    # Extract respective rows in buses to determine coordinates\n",
    "    buses_s1 = curated_buses.loc[curated_buses.name == s1]\n",
    "    buses_s2 = curated_buses.loc[curated_buses.name == s2]\n",
    "    \n",
    "    # If we were able to extract country from name, restrict chosen rows to this country.\n",
    "    if not pd.isna(fr_country):\n",
    "        buses_s1 = buses_s1.loc[buses_s1['country'] == fr_country]\n",
    "    if not pd.isna(to_country):\n",
    "        buses_s2 = buses_s2.loc[buses_s2['country'] == to_country]\n",
    "    \n",
    "    if buses_s1.empty or buses_s2.empty:\n",
    "        error_rows[index] = row\n",
    "        continue\n",
    "    \n",
    "    # Choose pair which matches length best\n",
    "    length = row['specified_length_km']\n",
    "    (x1, y1), (x2, y2), coord_dist = match_pair_with_length(buses_s1, buses_s2, length)\n",
    "        \n",
    "    tpl = (s1, x1, y1, s2, x2, y2, coord_dist)\n",
    "    \n",
    "    # TODO: how to choose an appropriate tolerance?\n",
    "    if math.isclose(coord_dist, length, rel_tol=0.45):\n",
    "        fr_to_tuples[index] = tpl\n",
    "    else:\n",
    "        error_rows[index] = row"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "coordinates = pd.DataFrame(index=fr_to_tuples.keys(), data=fr_to_tuples.values(), columns=['s1', 'x1', 'y1', 's2', 'x2', 'y2', 'coord_dist'])\n",
    "\n",
    "result = lines.copy()\n",
    "result = result.join(coordinates)\n",
    "\n",
    "percentage = coordinates.index.size / lines.index.size\n",
    "print(f'{percentage * 100}% of lines are probably correct.')\n",
    "\n",
    "# print('Lines where we probably found the correct coordinates:')\n",
    "# result.loc[~result.s1.isna()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "error_lines = result.loc[result.s1.isna()]\n",
    "error_subst = set(error_lines['substation_1']).union(error_lines['substation_2'])\n",
    "\n",
    "# print('')\n",
    "# {(k,tyndp_to_bus[k]) for k in error_subst}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Determine coordinates using geopy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def match_pair_with_length_geopy(s1_locations, s2_locations, length):\n",
    "    s1_first_name = s1_locations[0][0]\n",
    "    s2_first_name = s2_locations[0][0]\n",
    "\n",
    "    # Only take locations which at least include name of the first location in list (assumption: best name-based match).\n",
    "    s1_locations = [l for l in s1_locations if s1_first_name in l[0]]\n",
    "    s2_locations = [l for l in s2_locations if s2_first_name in l[0]]\n",
    "\n",
    "    return match_coord_pairs_with_length(s1_locations, s2_locations, length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def lat_lon(loc):\n",
    "    return (loc.latitude, loc.longitude)\n",
    "\n",
    "def match_coord_pairs_with_length(s1_coords, s2_coords, length):\n",
    "    combinations  = list(itertools.product(s1_coords, s2_coords))\n",
    "    with_distance = [(a, b, geopy.distance.distance(lat_lon(a),lat_lon(b)).km) for (a,b) in combinations]\n",
    "    \n",
    "    best_match = min(with_distance, key=lambda t: abs(length - t[2]))\n",
    "    return best_match"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# locator = Nominatim(user_agent='esm_group')\n",
    "# geocode = RateLimiter(locator.geocode, min_delay_seconds=0.01)\n",
    "locator = AlgoliaPlaces(user_agent='esm_group')\n",
    "geocode = locator.geocode"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fr_to_tuples_geopy = {}\n",
    "error_tuples_geopy = {}\n",
    "\n",
    "for index, row in error_lines.iterrows():\n",
    "    fr   = row['substation_1']\n",
    "    to   = row['substation_2']\n",
    "    dist = row['specified_length_km']\n",
    "\n",
    "    fr_country = row['country_1']\n",
    "    to_country = row['country_2']\n",
    "\n",
    "    # TODO: is it possible to get several matching locations?\n",
    "    fr_locs = geocode(fr, exactly_one=False) if pd.isna(fr_country) else geocode(fr, exactly_one=False, countries=[fr_country])\n",
    "    to_locs = geocode(to, exactly_one=False) if pd.isna(to_country) else geocode(to, exactly_one=False, countries=[to_country])\n",
    "    \n",
    "    if fr_locs is None or to_locs is None:\n",
    "        continue\n",
    "        \n",
    "    (s1, (x1, y1)), (s2, (x2, y2)), coord_dist = match_pair_with_length_geopy(fr_locs, to_locs, dist)\n",
    "    tpl = (s1, x1, y1, s2, x2, y2, coord_dist)\n",
    "\n",
    "    if not math.isclose(coord_dist, dist, rel_tol=0.45):\n",
    "        error_tuples_geopy[index] = tpl\n",
    "    else:\n",
    "        fr_to_tuples_geopy[index] = tpl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "coordinates_geopy = pd.DataFrame(index=fr_to_tuples_geopy.keys(), data=fr_to_tuples_geopy.values(), columns=['s1', 'x1', 'y1', 's2', 'x2', 'y2', 'coord_dist'])\n",
    "\n",
    "coordinates = coordinates.append(coordinates_geopy)\n",
    "\n",
    "result = lines.copy()\n",
    "result = result.join(coordinates)\n",
    "\n",
    "percentage = coordinates.index.size / lines.index.size\n",
    "print(f'{percentage * 100}% of lines are probably correct.')\n",
    "\n",
    "print('Lines where we probably found the correct coordinates:')\n",
    "result.loc[~result.s1.isna()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "error_coordinates_geopy = pd.DataFrame(index=error_tuples_geopy.keys(), data=error_tuples_geopy.values(), columns=['s1', 'x1', 'y1', 's2', 'x2', 'y2', 'coord_dist'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "error_coordinates_geopy.join(lines)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: plot error tuples and matches separately."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: write out data to csv\n",
    "# TODO: map commissioning years to consistent values"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1b82140c49c13528ccbaca54e46b6a97d4a943e8e9e85082a7581e75126fdf9"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('pypsa-eur': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}